---
layout: post
title: "FlashAttention: Solving the O(n^2) Bottleneck in Transformer Attention"
summary: "An introduction to FlashAttention â€” a memory-efficient and IO-aware algorithm that accelerates transformer attention while keeping results exact."
---

## The Problem: Standard Attention Is Memory-Hungry

## The Insight Behind FlashAttention

## Step-by-Step Overview

## FlashAttention v2 and Beyond
