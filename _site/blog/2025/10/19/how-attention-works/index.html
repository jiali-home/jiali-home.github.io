<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>So, how attention actually works? | Jia Li</title>
  <link rel="shortcut icon" href="/assets/images/logo.ico" type="image/x-icon">
  <link rel="stylesheet" href="/assets/css/style.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;500;600&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="/assets/css/rouge-github.css">
  <!-- GitHub Markdown CSS for GitHub-like rendering -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5.5.1/github-markdown-light.min.css">
  <script>
    // MathJax v3 configuration for inline ($...$) and display ($$...$$) math
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']]
      },
      svg: { fontCache: 'global' }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
  <style>
    /* Prevent global wide-screen flex from affecting post pages */
    .post-wrapper { max-width: 900px; margin: 2rem auto; padding: 0 1rem; display: block !important; }
    .markdown-body { background: transparent; color: var(--white-1); }
    /* Override global `article { display:none; }` from site CSS */
    .post-content { display: block !important; }
    .post-title { font-size: 2rem; margin-bottom: 0.25rem; }
    .post-date { color: #888; margin-bottom: 1.5rem; }
    .post-content p, .post-content li { line-height: 1.7; }
    /* Fix: global reset sets span { display:block } which breaks Rouge tokens */
    .post-content pre span,
    .post-content code span { display: inline !important; }
    .post-content pre { white-space: pre; }
    .post-content code { white-space: pre-wrap; word-break: normal; overflow-wrap: normal; }
    /* Headings spacing in posts */
    .post-content h2 { margin-top: 2rem; margin-bottom: 0.75rem; line-height: 1.3; }
    .post-content h3 { margin-top: 1.5rem; margin-bottom: 0.5rem; line-height: 1.35; }
    .post-content h4 { margin-top: 1.25rem; margin-bottom: 0.4rem; }
    /* Paragraph spacing */
    .post-content p { margin: 0 0 1rem; }
    /* Scope fixes so post Markdown matches GitHub expectations */
    .markdown-body a,
    .markdown-body img,
    .markdown-body span,
    .markdown-body button,
    .markdown-body time { display: inline; }
    /* Restore list bullets/numbers inside blog posts */
    .post-content ul li { list-style: disc; }
    .post-content ol li { list-style: decimal; }
    .post-content ul, .post-content ol { padding-left: 1.25rem; margin: 0 0 1rem; }
    .post-content li { margin: 0.25rem 0; }
    /* Code blocks */
    .post-content pre { background: var(--eerie-black-1); padding: 0.75rem 1rem; border-radius: 10px; overflow: auto; }
    .post-content code { font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace; font-size: 0.95em; }
    .post-content img { max-width: 100%; height: auto; }
    /* Simple vertical spacers usable via raw HTML in Markdown */
    .post-content .spacer-1 { height: 0.75rem; }
    .post-content .spacer-2 { height: 1.5rem; }
    .post-content .spacer-3 { height: 2rem; }
    .back-link { display: inline-block; margin-top: 2rem; color: #7c7cff; }
  </style>
</head>
<body>
  
  <main class="post-wrapper">
    <h1 class="post-title">So, how attention actually works?</h1>
    <div class="post-date">2025-10-19</div>
    <article class="post-content markdown-body">
      <p>When I first hear the term attention in deep learning, it sounds almost human — as if the model is “focusing” on certain parts of the input more than others. And that’s actually not too far from the truth. The attention mechanism allows a model to decide which pieces of information are most relevant when processing a sequence, whether that’s a sentence, an image, or even a series of actions.</p>

<h2 id="the-mechanics-query-key-and-value">The Mechanics: Query, Key, and Value</h2>

<p>The brilliance of the Transformer paper is how it formalizes this intuition into a simple mathematical framework using three vectors: Query (Q), Key (K), and Value (V).</p>

<ul>
  <li>Query (Q): What we’re looking for — the current word or token we want to understand in context.</li>
  <li>Key (K): What each word in the input offers — a representation of its “meaning” or identity.</li>
  <li>Value (V): The actual information that will be passed on once the attention weights are decided.</li>
</ul>

<h2 id="the-attention-equation">The Attention Equation</h2>

<p>Once we have these vectors, we can compute attention like this:</p>

\[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\]

<p>Let’s break this down:</p>

<ol>
  <li>
    <p><strong>Similarity Score:</strong><br />
$QK^T$ measures how much each <strong>Query</strong> aligns with each <strong>Key</strong> — basically, <em>how relevant is each input token to the one we’re currently looking at?</em></p>
  </li>
  <li>
    <p><strong>Scaling:</strong><br />
The division by $ \sqrt{d_k} $ keeps the values stable (prevents very large dot products when the vector dimension is large).</p>
  </li>
  <li>
    <p><strong>Softmax:</strong><br />
Converts the similarity scores into probabilities that sum to 1 — these are our <em>attention weights</em>.</p>
  </li>
  <li>
    <p><strong>Weighted Sum:</strong><br />
Multiply these weights with the <strong>Value (V)</strong> vectors to get a weighted representation — this is what the model will actually use as contextual information.</p>
  </li>
</ol>

<h2 id="but-where-do-q-k-and-v-come-from">But where Do Q, K, and V Come From?</h2>

<p>In the Transformer architecture, these vectors aren’t given magically — they’re <em>learned linear projections</em> derived from the <strong>input embeddings</strong>.</p>

<p>Let’s go step by step:</p>

<hr />

<h4 id="1-start-with-input-embeddings">1. <strong>Start with Input Embeddings</strong></h4>

<p>Each token (word, subword, or symbol) in your sequence — say, <em>“The”</em>, <em>“cat”</em>, <em>“sat”</em> — is first converted into an <strong>embedding vector</strong>.<br />
If your model’s hidden size is 512, then each word is represented as a 512-dimensional vector.</p>

<p>So, suppose we have an input sequence:</p>

\[X = [x_1, x_2, x_3, \dots, x_n]\]

<p>Each $ x_i \in \mathbb{R}^{d_{\text{model}}} $ (for example, $ d_{\text{model}} = 512 $).</p>

<hr />

<h4 id="2-project-embeddings-to-q-k-and-v">2. <strong>Project Embeddings to Q, K, and V</strong></h4>

<p>For each token embedding $ x_i $, the model learns three <em>different linear transformations</em>:</p>

\[Q = XW_Q, \quad K = XW_K, \quad V = XW_V\]

<p>where:</p>

<ul>
  <li>$ W_Q, W_K, W_V \in \mathbb{R}^{d_{\text{model}} \times d_k} $ are <strong>learned weight matrices</strong>, trained end-to-end with the rest of the model.</li>
  <li>$ d_k $ is typically smaller than $ d_{\text{model}} $ (e.g., 64 if $ d_{\text{model}} = 512 $ and there are 8 attention heads).</li>
</ul>

<p>This means:</p>
<ul>
  <li>Each <strong>token embedding</strong> gets mapped into <strong>three distinct vector spaces</strong> — one for queries, one for keys, and one for values.</li>
  <li>These projections let the model learn different ways to compare and extract information.</li>
</ul>

<hr />

<h4 id="3-where-this-happens-in-the-transformer">3. <strong>Where This Happens in the Transformer</strong></h4>

<ul>
  <li>In the <strong>encoder</strong>, Q, K, and V all come from the same source sequence → <em>self-attention</em>.</li>
</ul>

<p>Self-attention allows each token in a sequence to attend to <em>other tokens in the same sequence</em>.
If your input sentence is:</p>
<blockquote>
  <p>“The cat sat on the mat.”</p>
</blockquote>

<p>When encoding the word <em>“sat”</em>, the model can look at <em>“cat”</em> to understand that the subject doing the action is <em>“cat”</em>.</p>

<p>So every token (like <em>“sat”</em>) forms its own <strong>Query</strong>, compares it with <strong>Keys</strong> from <em>all</em> tokens (including itself), and blends their <strong>Values</strong> based on relevance.</p>

\[Q, K, V = \text{from the same input sequence (X)}\]

<p>This is why it’s called <strong>self-attention</strong> — the model is attending to <em>itself</em>.</p>

<ul>
  <li>In the <strong>decoder</strong>, the first attention block is self-attention again, but the second uses:
    <ul>
      <li>Q from the decoder,</li>
      <li>K and V from the encoder outputs → <em>cross-attention</em>.</li>
    </ul>
  </li>
</ul>

<p>In the <strong>decoder</strong>, self-attention is <em>masked</em> — the model only attends to previous positions (not future words).<br />
This preserves the <strong>auto-regressive</strong> property (so it doesn’t “peek ahead” when generating text).</p>

<p>Cross-Attention Formula
\(Q = \text{from decoder}, \quad K = \text{from encoder}, \quad V = \text{from encoder}\)
The decoder sends out a <strong>Query</strong> like:</p>
<blockquote>
  <p>“I’m about to generate the next word — what parts of the input sentence should I focus on?”</p>
</blockquote>

<p>And the encoder responds with its <strong>Keys</strong> and <strong>Values</strong> that represent the meaning of the entire input sentence.</p>

<h2 id="what-is-multi-head-attention">What Is Multi-Head Attention?</h2>

<p>So far, we’ve talked about a <strong>single attention mechanism</strong> — one set of $ Q, K, V $ vectors producing one set of attention weights.<br />
But the Transformer paper (<em>“Attention Is All You Need”</em>) discovered that using <strong>multiple attention “heads”</strong> in parallel makes the model <em>much more powerful</em>.</p>

<h3 id="the-core-idea">The Core Idea</h3>

<p>Instead of learning <strong>one</strong> attention distribution, we learn <strong>several different ones simultaneously</strong> — each head focuses on a <em>different type of relationship</em> between tokens.</p>

<p>Mathematically:</p>

\[Q_i = XW_Q^{(i)}, \quad K_i = XW_K^{(i)}, \quad V_i = XW_V^{(i)}\]

\[\text{head}_i = \text{softmax}\left(\frac{Q_i K_i^T}{\sqrt{d_k}}\right)V_i\]

<p>Then combine:</p>

\[\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h)W_O\]

<p>where $ W_O $ is a learned projection back to the model dimension.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># attention_modules.py
</span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>


<span class="c1"># ---------------------------
# Utilities
# ---------------------------
</span>
<span class="k">def</span> <span class="nf">make_causal_mask</span><span class="p">(</span><span class="n">seq_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="s">"""
    Returns a [1, 1, seq_len, seq_len] upper-triangular causal mask with -inf above diagonal.
    Add this to attention scores BEFORE softmax.
    """</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">full</span><span class="p">((</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="s">"-inf"</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">triu</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (L, L) upper triangle set to -inf
</span>    <span class="k">return</span> <span class="n">mask</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>        <span class="c1"># (1, 1, L, L)
</span>

<span class="k">def</span> <span class="nf">apply_padding_mask</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="p">):</span>
    <span class="s">"""
    scores: (B, H, QL, KL)
    key_padding_mask: (B, KL) with True where positions are PAD/should be masked
    Returns scores with -inf where keys are padding.
    """</span>
    <span class="k">if</span> <span class="n">key_padding_mask</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">scores</span>
    <span class="c1"># Expand to (B, 1, 1, KL) then broadcast
</span>    <span class="n">mask</span> <span class="o">=</span> <span class="n">key_padding_mask</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="p">:].</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">bool</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="p">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s">"-inf"</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">scores</span>


<span class="k">def</span> <span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
    <span class="s">"""
    Q: (B, H, QL, D)
    K: (B, H, KL, D)
    V: (B, H, KL, D)
    attn_mask: (1 or B, 1 or H, QL, KL) additive mask with 0 for keep and -inf for block
    key_padding_mask: (B, KL) boolean, True=mask (ignore)
    returns: (context, attn_weights)
      context: (B, H, QL, D)
      attn_weights: (B, H, QL, KL)
    """</span>
    <span class="n">d_k</span> <span class="o">=</span> <span class="n">Q</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">d_k</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>  <span class="c1"># (B, H, QL, KL)
</span>
    <span class="k">if</span> <span class="n">attn_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="c1"># Broadcast-compatible additive mask
</span>        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">+</span> <span class="n">attn_mask</span>

    <span class="n">scores</span> <span class="o">=</span> <span class="n">apply_padding_mask</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="p">)</span>

    <span class="n">attn</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dropout_p</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">dropout_p</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">Q</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>

    <span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>  <span class="c1"># (B, H, QL, D)
</span>    <span class="k">return</span> <span class="n">context</span><span class="p">,</span> <span class="n">attn</span>


<span class="c1"># ---------------------------
# (1) Single-Head Self-Attention
# ---------------------------
</span>
<span class="k">class</span> <span class="nc">SingleHeadSelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Single-head self-attention.
    Inputs:
      x: (B, L, d_model)
      key_padding_mask: (B, L) boolean, True=pad (optional)
      causal: bool — apply causal mask (optional, default False)
    Returns:
      y: (B, L, d_model_head)  # equals head_dim
      attn: (B, 1, L, L)
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">causal</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">q</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (B, 1, L, D)
</span>        <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">k</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (B, 1, L, D)
</span>        <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (B, 1, L, D)
</span>
        <span class="n">attn_mask</span> <span class="o">=</span> <span class="n">make_causal_mask</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)</span> <span class="k">if</span> <span class="n">causal</span> <span class="k">else</span> <span class="bp">None</span>
        <span class="n">context</span><span class="p">,</span> <span class="n">attn</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="n">attn_mask</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="o">=</span><span class="n">key_padding_mask</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">attn_dropout</span>
        <span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">context</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (B, L, D)
</span>        <span class="k">return</span> <span class="n">y</span><span class="p">,</span> <span class="n">attn</span>  <span class="c1"># attn: (B, 1, L, L)
</span>

<span class="c1"># ---------------------------
# (2) Single-Head Cross-Attention
# ---------------------------
</span>
<span class="k">class</span> <span class="nc">SingleHeadCrossAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Single-head cross-attention.
    Inputs:
      q_inp: (B, Lq, d_model)      # e.g., decoder states
      kv_inp: (B, Lk, d_model)     # e.g., encoder outputs (memory)
      key_padding_mask: (B, Lk) boolean, True=pad (optional)
    Returns:
      y: (B, Lq, head_dim)
      attn: (B, 1, Lq, Lk)
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q_inp</span><span class="p">,</span> <span class="n">kv_inp</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">Lq</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">q_inp</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">Lk</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">kv_inp</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">q</span><span class="p">(</span><span class="n">q_inp</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (B, 1, Lq, D)
</span>        <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">k</span><span class="p">(</span><span class="n">kv_inp</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># (B, 1, Lk, D)
</span>        <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">(</span><span class="n">kv_inp</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># (B, 1, Lk, D)
</span>
        <span class="n">context</span><span class="p">,</span> <span class="n">attn</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="o">=</span><span class="n">key_padding_mask</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">attn_dropout</span>
        <span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">context</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (B, Lq, D)
</span>        <span class="k">return</span> <span class="n">y</span><span class="p">,</span> <span class="n">attn</span>  <span class="c1"># (B, 1, Lq, Lk)
</span>

<span class="c1"># ---------------------------
# (3) Multi-Head Self-Attention
# ---------------------------
</span>
<span class="k">class</span> <span class="nc">MultiHeadSelfAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Multi-head self-attention with output projection.
    Inputs:
      x: (B, L, d_model)
      key_padding_mask: (B, L) boolean, True=pad (optional)
      causal: bool (optional)
    Returns:
      y: (B, L, d_model)
      attn: (B, H, L, L)
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">out_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">head_dim</span>
        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">head_dim</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">inner_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">inner_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">inner_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">inner_dim</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">out_dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_reshape_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="c1"># (B, L, H*D) -&gt; (B, H, L, D)
</span>        <span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">t</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">causal</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_reshape_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">q</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_reshape_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">k</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_reshape_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="c1"># Q,K,V: (B, H, L, D)
</span>
        <span class="n">attn_mask</span> <span class="o">=</span> <span class="n">make_causal_mask</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)</span> <span class="k">if</span> <span class="n">causal</span> <span class="k">else</span> <span class="bp">None</span>
        <span class="n">context</span><span class="p">,</span> <span class="n">attn</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="n">attn_mask</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="o">=</span><span class="n">key_padding_mask</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">attn_dropout</span>
        <span class="p">)</span>  <span class="c1"># context: (B,H,L,D)
</span>
        <span class="c1"># Merge heads: (B, L, H*D)
</span>        <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="n">contiguous</span><span class="p">().</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">out_dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">out</span><span class="p">(</span><span class="n">context</span><span class="p">))</span>  <span class="c1"># (B, L, d_model)
</span>        <span class="k">return</span> <span class="n">y</span><span class="p">,</span> <span class="n">attn</span>  <span class="c1"># (B,H,L,L)
</span>

<span class="c1"># ---------------------------
# (4) Multi-Head Cross-Attention
# ---------------------------
</span>
<span class="k">class</span> <span class="nc">MultiHeadCrossAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""
    Multi-head cross-attention with output projection.
    Inputs:
      q_inp: (B, Lq, d_model)      # e.g., decoder states
      kv_inp: (B, Lk, d_model)     # e.g., encoder outputs (memory)
      key_padding_mask: (B, Lk) boolean, True=pad (optional)
    Returns:
      y: (B, Lq, d_model)
      attn: (B, H, Lq, Lk)
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">attn_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">out_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">head_dim</span>
        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">head_dim</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">inner_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">inner_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">inner_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">inner_dim</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">attn_dropout</span> <span class="o">=</span> <span class="n">attn_dropout</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">out_dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_reshape_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="c1"># (B, L, H*D) -&gt; (B, H, L, D)
</span>        <span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">t</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q_inp</span><span class="p">,</span> <span class="n">kv_inp</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">Lq</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">q_inp</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">Lk</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">kv_inp</span><span class="p">.</span><span class="n">shape</span>

        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_reshape_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">q</span><span class="p">(</span><span class="n">q_inp</span><span class="p">))</span>     <span class="c1"># (B,H,Lq,D)
</span>        <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_reshape_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">k</span><span class="p">(</span><span class="n">kv_inp</span><span class="p">))</span>    <span class="c1"># (B,H,Lk,D)
</span>        <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_reshape_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">v</span><span class="p">(</span><span class="n">kv_inp</span><span class="p">))</span>    <span class="c1"># (B,H,Lk,D)
</span>
        <span class="n">context</span><span class="p">,</span> <span class="n">attn</span> <span class="o">=</span> <span class="n">scaled_dot_product_attention</span><span class="p">(</span>
            <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="o">=</span><span class="n">key_padding_mask</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">attn_dropout</span>
        <span class="p">)</span>  <span class="c1"># context: (B,H,Lq,D)
</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="n">contiguous</span><span class="p">().</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">Lq</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">out_dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">out</span><span class="p">(</span><span class="n">context</span><span class="p">))</span>  <span class="c1"># (B, Lq, d_model)
</span>        <span class="k">return</span> <span class="n">y</span><span class="p">,</span> <span class="n">attn</span>  <span class="c1"># (B,H,Lq,Lk)
</span>
</code></pre></div></div>

    </article>
    <a class="back-link" href="/blog/">← Back to blog</a>
    <br>
    <a class="back-link" href="/index.html">← Back to home</a>
  </main>
</body>
</html>
